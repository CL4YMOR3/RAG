# HO RAG - Enterprise Hybrid RAG System

A Retrieval-Augmented Generation system with **Hybrid Search**, **HyDE**, **Reranking**, and **Inline Citations**.

## âœ¨ Features

| Feature | Description |
|---------|-------------|
| **Hybrid Search** | Dense (MiniLM) + Sparse (BM42) vectors for semantic + keyword matching |
| **HyDE** | Hypothetical Document Embeddings for improved retrieval |
| **Parent-Child Indexing** | Search on small chunks, retrieve full context |
| **FlashRank Reranking** | CPU-optimized reranking for precision |
| **Inline Citations** | `[Source: file.pdf, Pg X]` format in responses |
| **Conversational Memory** | Redis-backed session persistence |
| **Multi-Format Support** | PDF, DOCX, PPTX, XLSX, TXT, HTML, EPUB, RTF |
| **VRAM Protection** | Embeddings on CPU, only LLM on GPU |
| **Crash-Proof** | PostgreSQL (Neon) + Redis for persistent storage |

---

## ğŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         Frontend (React)                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                             â”‚ HTTP
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      FastAPI Backend                            â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚   /ingest/   â”‚  â”‚   /query/    â”‚  â”‚  /teams/ Management  â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚         â”‚                 â”‚                                     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚  Ingestion   â”‚  â”‚            Query Pipeline            â”‚    â”‚
â”‚  â”‚   Service    â”‚  â”‚  Router â†’ HyDE â†’ Search â†’ Rerank â†’   â”‚    â”‚
â”‚  â”‚              â”‚  â”‚  Generate w/ Citations               â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚         â”‚                              â”‚                        â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚                    Qdrant (Vector DB)                   â”‚   â”‚
â”‚  â”‚              Dense + Sparse Named Vectors               â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                                                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚              Qwen 2.5 3B (llama-cpp, GPU)               â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                                                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚     Redis (Cache)       â”‚  â”‚  PostgreSQL/Neon           â”‚   â”‚
â”‚  â”‚  - Session Memory       â”‚  â”‚  - Parent Document Storage â”‚   â”‚
â”‚  â”‚  - Parent Doc Cache     â”‚  â”‚  - Crash-proof backup      â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ“ Project Structure

```
HO RAG/
â”œâ”€â”€ backend/
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”œâ”€â”€ api.py              # FastAPI endpoints
â”‚   â”‚   â”œâ”€â”€ services/
â”‚   â”‚   â”‚   â”œâ”€â”€ ingestion.py    # Hybrid ingestion pipeline
â”‚   â”‚   â”‚   â”œâ”€â”€ query.py        # HyDE + Hybrid search + Citations
â”‚   â”‚   â”‚   â”œâ”€â”€ llm.py          # Qwen LLM service (Singleton factory)
â”‚   â”‚   â”‚   â”œâ”€â”€ router.py       # Semantic router (chat vs RAG)
â”‚   â”‚   â”‚   â””â”€â”€ chunking.py     # Text chunking strategies
â”‚   â”‚   â”œâ”€â”€ core/
â”‚   â”‚   â”‚   â””â”€â”€ config.py       # Settings & prompts
â”‚   â”‚   â””â”€â”€ db/
â”‚   â”‚       â”œâ”€â”€ vector_store.py # Qdrant connection
â”‚   â”‚       â”œâ”€â”€ postgres.py     # PostgreSQL/Neon parent storage
â”‚   â”‚       â””â”€â”€ redis_cache.py  # Redis caching & sessions
â”‚   â”œâ”€â”€ SLM/                    # Local LLM model files
â”‚   â”œâ”€â”€ .env.example            # Environment template
â”‚   â”œâ”€â”€ requirements.txt
â”‚   â””â”€â”€ docker-compose.yml      # Qdrant + Redis containers
â”‚
â””â”€â”€ frontend/                   # React UI
```

---

## ğŸš€ Quick Start

### 1. Configure Environment
```bash
cd backend
cp .env.example .env
# Edit .env with your Neon PostgreSQL connection string
```

### 2. Start Services (Qdrant + Redis)
```bash
docker-compose up -d
```

### 3. Install Dependencies
```bash
pip install -r requirements.txt
```

### 4. Start Backend
```bash
uvicorn src.api:app --reload --host 0.0.0.0 --port 8000
```

### 5. Start Frontend
```bash
cd frontend
npm install
npm start
```
### 5. Create folder for models
mkdir -p backend/SLM

# Download Qwen 2.5 3B (GGUF Quantized)
huggingface-cli download Qwen/Qwen2.5-3B-Instruct-GGUF qwen2.5-3b-instruct-q4_k_m.gguf --local-dir backend/SLM --local-dir-use-symlinks False
---

## ğŸ“¡ API Endpoints

| Method | Endpoint | Description |
|--------|----------|-------------|
| `POST` | `/ingest/` | Upload and index documents |
| `POST` | `/query/` | Query with RAG (non-streaming) |
| `POST` | `/query/stream` | Query with streaming response |
| `GET` | `/teams/` | List all teams/collections |
| `GET` | `/teams/{team}/documents` | List documents in team |
| `DELETE` | `/teams/{team}` | Delete team collection |
| `DELETE` | `/session/{session_id}` | Clear chat memory |

---

## âš™ï¸ Configuration

Create a `.env` file in the `backend/` directory (see `.env.example`):

| Variable | Default | Description |
|----------|---------|-------------|
| `DATABASE_URL` | (required) | PostgreSQL/Neon connection string |
| `REDIS_URL` | `redis://localhost:6379` | Redis connection URL |
| `REDIS_SESSION_TTL` | `86400` | Session expiry (24 hours) |
| `REDIS_CACHE_TTL` | `604800` | Cache expiry (7 days) |
| `QDRANT_HOST` | `localhost` | Qdrant server host |
| `QDRANT_PORT` | `6333` | Qdrant server port |
| `LLM_MODEL_PATH` | `SLM/Qwen2.5-3B-Instruct/...` | Path to GGUF model |
| `CONTEXT_WINDOW` | `4096` | LLM context size (4096 for 4GB VRAM) |
| `N_GPU_LAYERS` | `-1` | GPU layers (-1 = all) |

---

## ğŸ§ª How It Works

### Ingestion Pipeline
1. **Load Document** â†’ Extract text from PDF/DOCX/etc
2. **Parent-Child Chunking** â†’ 2000 char parents, 400 char children
3. **Generate Embeddings** â†’ Dense (MiniLM) + Sparse (SPLADE)
4. **Index to Qdrant** â†’ Named vectors for hybrid search

### Query Pipeline
1. **Route** â†’ Chat vs RAG classification
2. **Contextualize** â†’ Rewrite query with history (Redis)
3. **HyDE** â†’ Generate hypothetical answer
4. **Hybrid Search** â†’ RRF fusion of dense + sparse
5. **Fetch Parents** â†’ Redis cache â†’ PostgreSQL fallback
6. **Rerank** â†’ FlashRank top 3
7. **Generate** â†’ Qwen with citation enforcement

---

## ğŸ¤– LLM Service Architecture

The `LocalQwenGPU` class provides a custom LlamaIndex `CustomLLM` implementation:

| Component | Description |
|-----------|-------------|
| **Singleton Factory** | `LLMService.get_llm()` ensures single model instance |
| **ChatML Format** | Proper `<\|im_start\|>` / `<\|im_end\|>` prompt formatting |
| **Dual Callbacks** | `@llm_completion_callback()` for complete, `@llm_chat_callback()` for chat |
| **GPU Acceleration** | Full GPU offload via `llama-cpp-python` (`n_gpu_layers=-1`) |
| **Streaming Support** | Both `stream_complete()` and `stream_chat()` methods |

```python
from src.services.llm import LLMService

# Get singleton LLM instance
llm = LLMService.get_llm()

# Use for completions or chat
response = llm.complete("Your prompt here")
```

---


